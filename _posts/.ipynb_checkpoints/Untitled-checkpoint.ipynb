{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps come with a certain caveat. \n",
    "\n",
    "We modify and delete features that have the same value for all samples (so the variance of these individual features = 0) or that have 'special' values. This is valid for one dataset (e.g. train), but might look completely different for the valid or even the test dataset. As a conclusion the outcome of the operations have to be applied to the valid and test set too. Even though the variance might NOT be = 0 there or the 'special' values are distributed differently etc. \n",
    "\n",
    "All this might impact the final model. It might be worth it to double check what the model would look like without these preparations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing 'special' values or missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...no attributes with missing values found\n"
     ]
    }
   ],
   "source": [
    "res = X_train.isnull().sum()\n",
    "if (res[res > 0].empty):\n",
    "    print('...no attributes with missing values found')\n",
    "else:\n",
    "    print('...found {0} attributes with missing values'.format(res))\n",
    "\n",
    "# analyze attributes manually that have 'fillers' such as -999999 and replace fillers with new value\n",
    "# in the examples below the 75% percentile has been taken as a new value to replace fillers\n",
    "X_train['var3'].replace(to_replace=-999999, value=2, inplace=True)\n",
    "X_train['delta_imp_amort_var18_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_imp_amort_var34_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_imp_aport_var13_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_imp_aport_var17_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_imp_compra_var44_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_imp_reemb_var13_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_imp_reemb_var17_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_imp_reemb_var33_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_imp_trasp_var17_in_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_imp_trasp_var17_out_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_imp_trasp_var33_in_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_imp_trasp_var33_out_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_imp_venta_var44_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_num_aport_var13_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_num_aport_var17_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_num_aport_var33_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_num_compra_var44_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_num_reemb_var13_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_num_reemb_var17_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_num_reemb_var33_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_num_trasp_var17_in_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_num_trasp_var17_out_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_num_trasp_var33_in_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_num_trasp_var33_out_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_train['delta_num_venta_var44_1y3'].replace(to_replace=9999999999, value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete features with zero variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - deleted 44 / 370 features (~= 11.9 %)\n"
     ]
    }
   ],
   "source": [
    "n_features_originally = X_train.shape[1]\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(data)\n",
    "\n",
    "orig_feat_ix = np.arange(data.columns.size)\n",
    "feat_ix_keep = selector.get_support(indices=True)\n",
    "feat_ix_delete = np.delete(orig_feat_ix, feat_ix_keep)\n",
    "\n",
    "X_train.drop(labels=data.columns[feat_ix_delete], axis=1, inplace=True)\n",
    "\n",
    "n_features_deleted = feat_ix_delete.size\n",
    "\n",
    "print(' - deleted %s / %s features (~= %.1f %%)' % (n_features_deleted, n_features_originally,\n",
    "      100.0 * (np.float(n_features_deleted) / n_features_originally)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete identical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Deleted 50 / 326 features\n"
     ]
    }
   ],
   "source": [
    "n_features_originally = X_train.shape[1]\n",
    "\n",
    "feat_names_delete = []\n",
    "for feat_1, feat_2 in itertools.combinations(iterable=X_train.columns, r=2):\n",
    "    if np.array_equal(X_train[feat_1], X_train[feat_2]):\n",
    "        feat_names_delete.append(feat_2)\n",
    "\n",
    "feat_names_delete = np.unique(feat_names_delete)\n",
    "\n",
    "X_train.drop(labels=feat_names_delete, axis=1, inplace=True)\n",
    "\n",
    "n_features_deleted = len(feat_names_delete)\n",
    "\n",
    "print(' - Deleted %s / %s features' % (n_features_deleted, n_features_originally))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare valid set accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply exactly the same steps as above to the valid data set. But this time without analyzing it as the results might look different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read valid dataset\n",
    "X_valid = pd.read_csv('X_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imput missing values and replace 'special' values\n",
    "X_valid['var3'].replace(to_replace=-999999, value=2, inplace=True)\n",
    "X_valid['delta_imp_amort_var18_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_imp_amort_var34_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_imp_aport_var13_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_imp_aport_var17_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_imp_compra_var44_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_imp_reemb_var13_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_imp_reemb_var17_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_imp_reemb_var33_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_imp_trasp_var17_in_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_imp_trasp_var17_out_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_imp_trasp_var33_in_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_imp_trasp_var33_out_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_imp_venta_var44_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_num_aport_var13_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_num_aport_var17_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_num_aport_var33_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_num_compra_var44_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_num_reemb_var13_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_num_reemb_var17_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_num_reemb_var33_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_num_trasp_var17_in_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_num_trasp_var17_out_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_num_trasp_var33_in_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_num_trasp_var33_out_1y3'].replace(to_replace=9999999999, value=0, inplace=True)\n",
    "X_valid['delta_num_venta_var44_1y3'].replace(to_replace=9999999999, value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop zero variance features according to X_train outcome\n",
    "X_valid.drop(labels=data.columns[feat_ix_delete], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop identical features according to X_train outcome\n",
    "X_valid.drop(labels=feat_names_delete, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write reduced train and valid data into separat .csv files for further processing\n",
    "X_train.to_csv(\"X_train_reduced.csv\", sep=',', header=True)\n",
    "X_valid.to_csv(\"X_valid_reduced.csv\", sep=',', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
