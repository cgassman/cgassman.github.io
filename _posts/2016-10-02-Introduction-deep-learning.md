---
layout: post
title: "Introduction deep learning"
date: 2016-10-02
---

This blog is the result of a writing assignment during my data science certification. It summarizes about 40 pages of primary literature on deep learning. I chose this topic to gain first insights in deep learning. The primary literature used is referenced at the bottom of this page. 

## Management Abstract
Deep learning is a specific machine learning technique. It enables machine learning in areas where conventional methods have limited success, such as in processing images, speech or audio.     

The key difference between conventional machine learning and deep learning concerns feature 'construction'. In conventional machine learning, features are human engineered. In deep learning, features are learned automatically: out of the raw-data over several abstraction layers, via computational models. 

The computational models approximate (complex) non-linear functions and are implemented as deep-graphs - graphs with many, so called hidden layers, put between an input and an output layer. This under the assumption, that "anything" (above quantum physics) can be expressed with a non-linear function. The topology of the selected graph (e.g. how the nodes are connected, how the nodes are structured into layers), the characteristics of each node in the graph, as well as the weights on the connections between the nodes of the graph and a set of hyper-parameters define the computational model's design. Their role and impact are explained in this assignment.        

Compared to machines, humans are quick learners and need only a few examples to learn something new. Machines on the other side are slow learners and need billions of examples to achieve the ‘same result’. Therefore, it is important to have (computationally) efficient models in place to train machines with the support of deep-learning.      

Summarized: Deep learning is used to approximate complex non-linear functions through automated feature construction. The approximation and therefore also the features are computed by efficient computational models that can be represented through deep-graphs.   
  
## Introduction und structure
Coming from the area of conventional machine learning this writing assignment enables first steps in the area of deep learning: 
Starting from the [1] “Deep Learning” article to get a first overview, proceeding via internet searches for related literature ([3], [5], [6] and [7]), as well as for deep learning hands-on tutorials [2]. Mathematical background and explanations are kept on a minimum; instead, the focus lies on key terms and concepts. Many deep learning introductions seem to delve into mathematical explanations and notations very quickly, making it somewhat difficult to follow the key concepts. 

Various components and their tasks in a neural network are explained and set into context in the high-level graphical overview below. A real world coding example or hands-on tutorial has been postponed and is not part of this assignment.     

Relevant resources for this writing assignment are listed in the references at the bottom of this page.

## Summary of primary literature
Today, deep learning is often applied in the area of image-, video-, speech and audio- processing.  This is mainly because it might be quite complex to (manually) engineer features out of images, videos etc. from which a machine can learn. Hence, in deep learning the machine learns features automatically, they are not designed by human engineers, as in conventional machine learning techniques; instead, features are learned from data using a general-purpose learning procedure. The learning itself is an appliance of mathematical, computational models and an optimization of respective results.  

The computational models are composed of multiple processing layers (one input-, one output-, and zero to many hidden-layers) to learn representations of data with multiple levels of abstraction. Such abstraction levels can be obtained by composing simple but non-linear modules and each transforms the representation at one level: Starting from the raw-input (e.g. pixels of an image) into a representation at a higher, slightly more abstract level (e.g. simple objects). With the composition of enough such transformations, very complex functions can be learned.

Going one step further, the following parameters influence computational models required for deep learning: 

* The __network type/architecture/topology__ defines the number of layers, the number of neurons on each layer and the type of connections between neurons on different layers or to the neuron itself. The interconnections between neurons define which neurons can relay data to which other neurons. In general there is one input layer, one to many hidden layers and one output layer. However, different architectures produce very different network properties and possibilities. Examples of architectures are: convolutional networks or recurrent networks. A very illustrative overview on various network types can be found on [4][The Neural Network Zoo](http://www.asimovinstitute.org/neural-network-zoo/). 

* The __characteristics of individual neurons__ define the computation and therefore the activation function being used in a neuron. Typical activation functions used in deep learning are for example sigmoid, tanh, or rectified linear unit (ReLU). The neuron collects its different inputs, calculates their sum and runs the activation function on it. The so calculated result is used as output and serves as input into the next layer. The overall objective is to amplify signals and damp noise.      

* The __weight-coefficient__ defines how strong the connection between two neurons is weighted. In other words, the weight-coefficient describes how the output of one neuron weights as input into another neuron it is connected to. The weight-coefficient is used to calculate the collected and summarized input. The weight-coefficients are the parts that are adjusted during the training/learning epochs. After each epoch (pass over the training set) the optimal weight-coefficients are calculated. Optimal weight means that the cost function is optimized, as explained below. The initialization of weight-coefficients might become key regarding the speed how fast (or slow) the neural network learns. Especially saturated neurons - in the input layer but also in the hidden layers - slow down the process of learning. This is because saturated neurons (value close to 1 or 0) lead to small weight changes only and small weight-changes miniscule changes in the activation functions. As a result, the rest of the neurons is barely affected and the changes in the cost function are minuscule too, hence, weights only learn very slowly (i.e. if gradient descent is used as optimization-method). Therefore, the initialization of weight-coefficients should be chosen in a way that a saturation effect can be omitted and the learning slowdown can be avoided. One way to achieve this is by using sharply peaked distributions instead of a broad Gaussian distribution, as explained in [6][Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com).      

* The __cost-function__ and its __optimization-method__ define the function to be optimized (cost function or loss function) and how this optimization is done (optimization-method). The cost-function quantifies how close a given neural network is to the ideal it is training towards. Typical cost functions for classification problems - answering the question "what kind?" - are for example the Hinge Loss or the Logistic Loss function. For regression problems - answering the question "how much?" - The Mean Squared Error or the Mean Absolute Error are two examples of cost-functions. The cost-function might have a substantial impact on the way a network learns. A sub-optimal choice of the cost-function might slowdown the learning process, whereas ideal cost-functions that 'produce' large errors improve the speed a neuron will learn. Typical optimization methods are for example gradient descent, stochastic gradient descent or a special form of stochastic gradient descent, called mini-batch learning. Gradient descent calculates the overall loss across all of the training examples, calculates then the gradient and finally updates the weight-coefficients. Stochastic gradient descent calculates the gradient and the update on the weight-coefficients after every training sample. Mini-batch calculates the gradient and the update of the weight-coefficients after a set of training samples. The key of an optimization-method is that it can be done in a very effective way. Effective means that it doesn't use too much computational time and (hopefully) converges to an optimum. The backpropagation algorithm is often applied as a learning algorithm to achieve effectiveness; it is a computationally efficient approach to compute the derivatives of a (complex) cost-function. Based on the derivatives the weight-coefficients are learned. Instead of multiplying large matrices for each network layer (forward - from left to right), a matrix will be multiplied by a vector (backward - from right to left); A matrix vector computation is much cheaper than a matrix matrix multiplication. In other words: First, the activation of the output layer is obtained via forward propagation (from left to right), this means that the input features are propagated through the connections in the network. Second, the error vector of the output layer is calculated and is then back-propagated (from right to left), so that the weight-coefficients can be updated. In a short: backpropagation uses gradient descent on the weight-coefficients to minimize error on the output layer - "the neural network is being trained and learns". 
     
The following picture provides a simplified overview on key terms explained before and how they fit into the overall picture.     

![alt text](/assets/deep-learning-overview.png "deep learning overview")

## Conclusions
Deep learning - as one specific sub-area of machine learning - enables promising solutions in areas such as image- video- or language- processing: Features are learned automatically, instead of engineered manually as in conventional machine learning. However, neural networks bring along the difficulty to be set-up and adjusted in the “right way”. The choice of network (number of layers and number of neurons), cost-function and optimization-method, but also the tuning of (hyper)parameters (e.g. learning rate, regularization, weight-coefficient initialization) can improve the network’s speed of learning in a way that slowing down the learning process can be omitted while keeping computational efficiency at the same time and getting good or excellent results. This variety makes it difficult to get started with deep learning, but the effort might be worth it, as current movements in the area of machine learning strongly indicate that neural networks will play an (even more) important role in the future.    


## References 
[1] LeCun, Y., Bengio, Y., & Hinton, G. (2015). *Deep learning*. NATURE, 436-444.    
[2] LISA lab. (6. September 2016). *Deep Learning 0.1 Documentation*. [Deep Learning Tutorials](http://deeplearning.net/tutorial/)    
[3] Raschka, S. (2015). *Python Machine Learning*. In S. Raschka, Python Machine Learning (chapter 2 / chapter 12). Birmingham: Packt Publishing.    
[4] Veen, F. v. (14. September 2014). *The Neural Network Zoo*.[The Neural Network Zoo](http://www.asimovinstitute.org/neural-network-zoo/)     
[5] Hearty, J. (2016). *Advanced Machine Learning with Python*. In J. Hearty, Advanced Machine Learning with Python (chapter 2, part 1). Birmingham: Packt Publishing.      
[6] Nielsen, M. (2016). *Neural Networks and Deep Learning*. [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com)       
[7] Gibson, A., Patterson, J. (2016). *Deep Learning*. United States of America: O'Reilly Media.    